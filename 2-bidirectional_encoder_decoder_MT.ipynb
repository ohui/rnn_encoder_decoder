{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hk6GEEZJqB3x"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# import tensorflow \n",
    "# print(tensorflow.__version__)\n",
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from keras.layers import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Input,Embedding, Bidirectional, RepeatVector, TimeDistributed, Concatenate, concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "09NODKrbqLV_",
    "outputId": "b99b0457-41bf-4083-89e1-74ab26f0d74f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents\n",
    "data = read_text(\"deu.txt\")\n",
    "deu_eng = to_lines(data)\n",
    "deu_eng = array(deu_eng)\n",
    "n_sentences = 10000\n",
    "deu_eng = deu_eng[:n_sentences,:2]\n",
    "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
    "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
    "for i in range(len(deu_eng)):\n",
    "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
    "    deu_eng[i,1] = deu_eng[i,1].lower()\n",
    "eng_l = []\n",
    "deu_l = []\n",
    "for i in range(len(deu_eng)):\n",
    "    eng_l.append(len(deu_eng[i,0].split())) \n",
    "    deu_l.append(len(deu_eng[i,1].split()))\n",
    "\n",
    "eng_length = max(eng_l)\n",
    "deu_length = max(deu_l)\n",
    "print (eng_length)\n",
    "print (deu_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EQ1R7b7vq-gK",
    "outputId": "3dc7be7f-5c96-4756-a58a-26aff77c80bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2220\n",
      "Deutch Vocabulary Size: 3603\n"
     ]
    }
   ],
   "source": [
    "def tokenization(lines):\n",
    "      tokenizer = Tokenizer()\n",
    "      tokenizer.fit_on_texts(lines)\n",
    "      return tokenizer\n",
    "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
    "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "         # integer encode sequences\n",
    "         seq = tokenizer.texts_to_sequences(lines)\n",
    "         # pad sequences with 0 values\n",
    "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "         #seq = np.expand_dims(seq, axis=2)\n",
    "         return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "lcQ5DY5_rEje",
    "outputId": "db3b179d-a3cd-467c-ee56-41b2ffc135e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training example\n",
      "['tom will hurry' 'tom wird sich beeilen']\n",
      "input German encoding:\n",
      " [  2  37  28 452   0   0   0   0   0]\n",
      "target English encoding:\n",
      " [[  1]\n",
      " [ 75]\n",
      " [179]\n",
      " [  0]\n",
      " [  0]]\n",
      "training shapes\n",
      "(9000, 9)\n",
      "(9000, 5, 1)\n",
      "testing shapes\n",
      "(1000, 9)\n",
      "(1000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test set\n",
    "train, test = train_test_split(deu_eng, test_size=0.1, random_state = 12)\n",
    "\n",
    "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = trainY.reshape((trainY.shape[0], trainY.shape[1], 1))\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = testY.reshape((testY.shape[0], testY.shape[1], 1))\n",
    "\n",
    "print (\"first training example\")\n",
    "print (train[0])\n",
    "print ('input German encoding:\\n', trainX[0])\n",
    "print ('target English encoding:\\n', trainY[0])\n",
    "\n",
    "print (\"training shapes\")\n",
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "\n",
    "print (\"testing shapes\")\n",
    "print (testX.shape)\n",
    "print (testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6PL86-wrFFA"
   },
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    x = Input(shape = (src_timesteps,))\n",
    "    encoder_inputs = Embedding(src_vocab+1, n_units, input_length=src_timesteps, mask_zero=True)(x)\n",
    "    #encoder = LSTM(n_units)(encoder_inputs)  # first LSTM layer is a many-to-one\n",
    "    encoder = Bidirectional(LSTM(n_units), merge_mode=\"sum\")(encoder_inputs)\n",
    "\n",
    "    encoder_output = RepeatVector(tar_timesteps)(encoder)\n",
    "    decoder_inputs = LSTM(n_units, return_sequences=True)(encoder_output) \n",
    "    decoder_outputs = TimeDistributed(Dense(tar_vocab, activation='softmax'))(decoder_inputs) # many-to-many type\n",
    "    model = Model(input = [x], output = [decoder_outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tcQVAgr3rKxT",
    "outputId": "afc0cd77-dceb-42d9-ff3d-d8d933c00d74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 9, 256)            922624    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 256)               1050624   \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 2220)           570540    \n",
      "=================================================================\n",
      "Total params: 3,069,100\n",
      "Trainable params: 3,069,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 11s 1ms/step - loss: 4.1232 - acc: 0.4413 - val_loss: 3.4017 - val_acc: 0.4586\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.40167, saving model to my-ed-model.h5\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 7s 815us/step - loss: 3.2547 - acc: 0.4685 - val_loss: 3.2605 - val_acc: 0.4720\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.40167 to 3.26048, saving model to my-ed-model.h5\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 7s 819us/step - loss: 3.0703 - acc: 0.4817 - val_loss: 3.0964 - val_acc: 0.4926\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.26048 to 3.09640, saving model to my-ed-model.h5\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 7s 814us/step - loss: 2.8437 - acc: 0.5176 - val_loss: 2.8957 - val_acc: 0.5326\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.09640 to 2.89575, saving model to my-ed-model.h5\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 8s 844us/step - loss: 2.6086 - acc: 0.5518 - val_loss: 2.7852 - val_acc: 0.5548\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.89575 to 2.78521, saving model to my-ed-model.h5\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 7s 812us/step - loss: 2.4075 - acc: 0.5786 - val_loss: 2.6060 - val_acc: 0.5762\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.78521 to 2.60599, saving model to my-ed-model.h5\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 2.2196 - acc: 0.6007 - val_loss: 2.4918 - val_acc: 0.5954\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.60599 to 2.49183, saving model to my-ed-model.h5\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 8s 851us/step - loss: 2.0506 - acc: 0.6218 - val_loss: 2.4012 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.49183 to 2.40120, saving model to my-ed-model.h5\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 8s 842us/step - loss: 1.8966 - acc: 0.6426 - val_loss: 2.3026 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.40120 to 2.30261, saving model to my-ed-model.h5\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 7s 818us/step - loss: 1.7398 - acc: 0.6630 - val_loss: 2.2195 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.30261 to 2.21948, saving model to my-ed-model.h5\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 7s 790us/step - loss: 1.5987 - acc: 0.6807 - val_loss: 2.1766 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.21948 to 2.17662, saving model to my-ed-model.h5\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 7s 790us/step - loss: 1.4615 - acc: 0.6982 - val_loss: 2.1006 - val_acc: 0.6530\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.17662 to 2.10056, saving model to my-ed-model.h5\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 7s 807us/step - loss: 1.3350 - acc: 0.7164 - val_loss: 2.0555 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.10056 to 2.05552, saving model to my-ed-model.h5\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 7s 802us/step - loss: 1.2180 - acc: 0.7359 - val_loss: 2.0201 - val_acc: 0.6608\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.05552 to 2.02015, saving model to my-ed-model.h5\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 7s 784us/step - loss: 1.1084 - acc: 0.7563 - val_loss: 1.9744 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.02015 to 1.97440, saving model to my-ed-model.h5\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 7s 814us/step - loss: 0.9998 - acc: 0.7765 - val_loss: 1.9501 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.97440 to 1.95012, saving model to my-ed-model.h5\n",
      "Epoch 17/50\n",
      "9000/9000 [==============================] - 7s 808us/step - loss: 0.9067 - acc: 0.7949 - val_loss: 1.9196 - val_acc: 0.6862\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.95012 to 1.91960, saving model to my-ed-model.h5\n",
      "Epoch 18/50\n",
      "9000/9000 [==============================] - 7s 797us/step - loss: 0.8186 - acc: 0.8125 - val_loss: 1.8899 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.91960 to 1.88988, saving model to my-ed-model.h5\n",
      "Epoch 19/50\n",
      "9000/9000 [==============================] - 7s 830us/step - loss: 0.7361 - acc: 0.8320 - val_loss: 1.8684 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.88988 to 1.86844, saving model to my-ed-model.h5\n",
      "Epoch 20/50\n",
      "9000/9000 [==============================] - 7s 803us/step - loss: 0.6569 - acc: 0.8511 - val_loss: 1.8448 - val_acc: 0.6974\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.86844 to 1.84479, saving model to my-ed-model.h5\n",
      "Epoch 21/50\n",
      "9000/9000 [==============================] - 7s 802us/step - loss: 0.5903 - acc: 0.8662 - val_loss: 1.8432 - val_acc: 0.7034\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.84479 to 1.84316, saving model to my-ed-model.h5\n",
      "Epoch 22/50\n",
      "9000/9000 [==============================] - 7s 818us/step - loss: 0.5274 - acc: 0.8813 - val_loss: 1.8269 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.84316 to 1.82693, saving model to my-ed-model.h5\n",
      "Epoch 23/50\n",
      "9000/9000 [==============================] - 7s 803us/step - loss: 0.4705 - acc: 0.8940 - val_loss: 1.8316 - val_acc: 0.7074\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.82693\n",
      "Epoch 24/50\n",
      "9000/9000 [==============================] - 7s 814us/step - loss: 0.4239 - acc: 0.9047 - val_loss: 1.8272 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.82693\n",
      "Epoch 25/50\n",
      "9000/9000 [==============================] - 7s 833us/step - loss: 0.3815 - acc: 0.9138 - val_loss: 1.8204 - val_acc: 0.7092\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.82693 to 1.82043, saving model to my-ed-model.h5\n",
      "Epoch 26/50\n",
      "9000/9000 [==============================] - 7s 800us/step - loss: 0.3420 - acc: 0.9229 - val_loss: 1.8160 - val_acc: 0.7132\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.82043 to 1.81604, saving model to my-ed-model.h5\n",
      "Epoch 27/50\n",
      "9000/9000 [==============================] - 7s 813us/step - loss: 0.3110 - acc: 0.9290 - val_loss: 1.8225 - val_acc: 0.7106\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.81604\n",
      "Epoch 28/50\n",
      "9000/9000 [==============================] - 7s 827us/step - loss: 0.2808 - acc: 0.9359 - val_loss: 1.8343 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.81604\n",
      "Epoch 29/50\n",
      "9000/9000 [==============================] - 7s 802us/step - loss: 0.2558 - acc: 0.9394 - val_loss: 1.8275 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.81604\n",
      "Epoch 30/50\n",
      "9000/9000 [==============================] - 7s 800us/step - loss: 0.2330 - acc: 0.9425 - val_loss: 1.8385 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.81604\n",
      "Epoch 31/50\n",
      "9000/9000 [==============================] - 7s 790us/step - loss: 0.2145 - acc: 0.9461 - val_loss: 1.8384 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.81604\n",
      "Epoch 32/50\n",
      "9000/9000 [==============================] - 7s 793us/step - loss: 0.1986 - acc: 0.9483 - val_loss: 1.8344 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.81604\n",
      "Epoch 33/50\n",
      "9000/9000 [==============================] - 7s 803us/step - loss: 0.1860 - acc: 0.9494 - val_loss: 1.8708 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.81604\n",
      "Epoch 34/50\n",
      "9000/9000 [==============================] - 7s 795us/step - loss: 0.1741 - acc: 0.9515 - val_loss: 1.8493 - val_acc: 0.7182\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.81604\n",
      "Epoch 35/50\n",
      "9000/9000 [==============================] - 7s 797us/step - loss: 0.1625 - acc: 0.9526 - val_loss: 1.8732 - val_acc: 0.7146\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.81604\n",
      "Epoch 36/50\n",
      "9000/9000 [==============================] - 7s 783us/step - loss: 0.1507 - acc: 0.9531 - val_loss: 1.8714 - val_acc: 0.7152\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.81604\n",
      "Epoch 37/50\n",
      "9000/9000 [==============================] - 7s 789us/step - loss: 0.1440 - acc: 0.9542 - val_loss: 1.8820 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.81604\n",
      "Epoch 38/50\n",
      "9000/9000 [==============================] - 7s 800us/step - loss: 0.1373 - acc: 0.9551 - val_loss: 1.8906 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.81604\n",
      "Epoch 39/50\n",
      "9000/9000 [==============================] - 7s 784us/step - loss: 0.1313 - acc: 0.9550 - val_loss: 1.9144 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.81604\n",
      "Epoch 40/50\n",
      "9000/9000 [==============================] - 7s 789us/step - loss: 0.1272 - acc: 0.9554 - val_loss: 1.8923 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.81604\n",
      "Epoch 41/50\n",
      "9000/9000 [==============================] - 7s 802us/step - loss: 0.1220 - acc: 0.9565 - val_loss: 1.9101 - val_acc: 0.7146\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.81604\n",
      "Epoch 42/50\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 0.1180 - acc: 0.9566 - val_loss: 1.9248 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.81604\n",
      "Epoch 43/50\n",
      "9000/9000 [==============================] - 7s 803us/step - loss: 0.1161 - acc: 0.9561 - val_loss: 1.9261 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.81604\n",
      "Epoch 44/50\n",
      "9000/9000 [==============================] - 7s 780us/step - loss: 0.1102 - acc: 0.9571 - val_loss: 1.9293 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.81604\n",
      "Epoch 45/50\n",
      "9000/9000 [==============================] - 7s 789us/step - loss: 0.1083 - acc: 0.9567 - val_loss: 1.9451 - val_acc: 0.7182\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.81604\n",
      "Epoch 46/50\n",
      "9000/9000 [==============================] - 7s 797us/step - loss: 0.1068 - acc: 0.9561 - val_loss: 1.9541 - val_acc: 0.7148\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.81604\n",
      "Epoch 47/50\n",
      "9000/9000 [==============================] - 7s 800us/step - loss: 0.1040 - acc: 0.9565 - val_loss: 1.9507 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.81604\n",
      "Epoch 48/50\n",
      "9000/9000 [==============================] - 7s 787us/step - loss: 0.1024 - acc: 0.9562 - val_loss: 1.9705 - val_acc: 0.7198\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.81604\n",
      "Epoch 49/50\n",
      "9000/9000 [==============================] - 7s 790us/step - loss: 0.1010 - acc: 0.9574 - val_loss: 1.9797 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.81604\n",
      "Epoch 50/50\n",
      "9000/9000 [==============================] - 7s 789us/step - loss: 0.0990 - acc: 0.9573 - val_loss: 1.9923 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.81604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19ecb7d550>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='my_model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'my-ed-model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "kFd2PWpr3ro4",
    "outputId": "4cb8af69-859c-4e75-e190-852ce5a97254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.953135\n",
      "BLEU-2: 0.937863\n",
      "BLEU-3: 0.860180\n",
      "BLEU-4: 0.536115\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def get_word(n, tokenizer):\n",
    "      for word, index in tokenizer.word_index.items():\n",
    "          if index == n:\n",
    "              return word\n",
    "      return None\n",
    "\n",
    "def prediction_translation(dataset, predictions, tokenizer):\n",
    "    actual = []\n",
    "    eng_predictions = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        curr_prediction = []\n",
    "        for j in range(len(predictions[i])):\n",
    "            best_index = argmax(predictions[i][j])\n",
    "            if best_index > 0:\n",
    "                curr_prediction.append(get_word(best_index, tokenizer))\n",
    "            else:\n",
    "                break\n",
    "        eng_predictions.append(curr_prediction)\n",
    "    \n",
    "    for t, s in dataset:\n",
    "        actual.append([t.split()])\n",
    "        \n",
    "    return actual, eng_predictions\n",
    "\n",
    "actual, predicted = prediction_translation(train, preds, eng_tokenizer)\n",
    "\n",
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jGyuilQHDjrV",
    "outputId": "b8e4b27b-b342-4563-bb9a-15eb338dce3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.577633\n",
      "BLEU-2: 0.460237\n",
      "BLEU-3: 0.374129\n",
      "BLEU-4: 0.177248\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(testX)\n",
    "actual, predicted = prediction_translation(test, preds, eng_tokenizer)\n",
    "\n",
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ulb5qf-WsurR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "my_ed_1 bidirectional.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
